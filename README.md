# 3-Layer-Deep-Neural-Network-for-Non-Linear-Regression

## Overview
This repository contains multiple implementations of a **3-layer deep neural network** for **non-linear regression** using **NumPy, PyTorch, and TensorFlow**. Each implementation explores different approaches, ranging from scratch implementations to high-level API usage.

### Key Features:
- **TensorFlow Einsum Usage**: TensorFlow implementations use `einsum` instead of traditional matrix multiplication.
- **3-Variable-Based Non-Linear Equation**: The dataset is generated based on a **three-variable equation**, unlike previous two-variable examples.
- **Synthetic Data Generation & 4D Plotting**: Data is generated and visualized in **4D**.
- **Full GitHub Documentation**: Includes **Colab links**, code explanations, and a **video walkthrough** for each implementation.
- **Detailed Video Walkthroughs**: Executed **Colab notebooks** are documented with step-by-step **code explanations**.

---

## üìä Data Generation

A **synthetic dataset** is generated using the following **3-variable-based non-linear equation**:

\[ y = \sin(x_1) + \cos(x_2) + x_3^2 \]

where **x‚ÇÅ, x‚ÇÇ, and x‚ÇÉ** are randomly sampled values between -1 and 1.

This dataset is used across all implementations for training and evaluation.

---

## üöÄ Implementations

### 1Ô∏è‚É£ NumPy-Only Implementation (Manual Backpropagation)

**File:** [colab_numpy_scratch.ipynb](./colab_numpy_scratch.ipynb)

üîπ Implements a **3-layer deep neural network from scratch** using **NumPy**.
üîπ Uses **non-linear activation functions**.
üîπ Implements **manual backpropagation** using the **chain rule**.
üîπ Displays **loss progression, training epochs, and final output**.

---

### 2Ô∏è‚É£ PyTorch Implementations

#### üîπ PyTorch Without Built-in Layers
**File:** [colab_pytorch_scratch.ipynb](./colab_pytorch_scratch.ipynb)

- **Implements the neural network manually** (no `nn.Module`, no built-in layers).
- Uses **manual backpropagation**.
- **Weight updates are handled manually** using gradients.

#### üîπ PyTorch Class-Based Model
**File:** [colab_pytorch_class.ipynb](./colab_pytorch_class.ipynb)

- Uses `nn.Module` and `nn.Linear` layers.
- Implements **forward propagation** and **automatic backpropagation**.
- Uses PyTorch's built-in **Adam optimizer**.

#### üîπ PyTorch Lightning Implementation
**File:** [colab_pytorch_lightning.ipynb](./colab_pytorch_lightning.ipynb)

- Uses **PyTorch Lightning** to manage training.
- Implements the model as a **LightningModule**.
- Uses **`Trainer`** for efficient training.

---

### 3Ô∏è‚É£ TensorFlow Implementations

#### üîπ TensorFlow Without High-Level APIs
**File:** [colab_tf_lowlevel.ipynb](./colab_tf_lowlevel.ipynb)

- Implements a **3-layer neural network manually** without using high-level TensorFlow functions.
- Uses **TensorFlow‚Äôs lower-level tensor operations**.

#### üîπ TensorFlow Built-in Layers
**File:** [colab_tf_builtin_layers.ipynb](./colab_tf_builtin_layers.ipynb)

- Uses **built-in layers** from TensorFlow (`tf.keras.layers.Dense`).
- Implements **automatic backpropagation and weight updates**.

#### üîπ TensorFlow Functional API
**File:** [colab_tf_functional_api.ipynb](./colab_tf_functional_api.ipynb)

- Implements the model using **TensorFlow‚Äôs Functional API**.
- More flexible than sequential models, allowing complex architectures.

#### üîπ TensorFlow High-Level API
**File:** [colab_tf_highlevel_api.ipynb](./colab_tf_highlevel_api.ipynb)

- Uses **high-level APIs** such as `tf.keras.Sequential`.
- Simplifies model definition and training.

---

### 4Ô∏è‚É£ TensorFlow Einsum Requirement
All TensorFlow implementations replace **matrix multiplication** with **`einsum`** to optimize computations.

Example:
```python
# Instead of: output = tf.matmul(X, W)
output = tf.einsum('ij,jk->ik', X, W)
```

---

### 5Ô∏è‚É£ Data Visualization (4D Plot)

- The **synthetic dataset** is **visualized using a 4D plot**.
- This helps analyze how data is distributed across multiple dimensions.
- **File:** `data_generation.py`

---

## üî• How to Run the Code

### Running in Google Colab

1. Open any of the `.ipynb` notebooks in **Google Colab**.
2. Run the cells step by step.
3. Observe the **loss progression**, **training epochs**, and **final output**.

### Running Locally

1. Clone the repository:
   ```bash
   git clone <repo-link>
   cd nn_non_linear_regression
   ```
2. Install dependencies:
   ```bash
   pip install torch torchvision torchaudio pytorch-lightning tensorflow numpy matplotlib
   ```
3. Run the notebooks in Jupyter Notebook or VS Code.

---

## üìΩÔ∏è Video Walkthroughs
Each implementation has an accompanying **video walkthrough**, explaining the code structure and execution.
- **[Watch NumPy Scratch Implementation Walkthrough](#)**
- **[Watch PyTorch Scratch Implementation Walkthrough](#)**
- **[Watch PyTorch Class-Based Walkthrough](#)**
- **[Watch PyTorch Lightning Walkthrough](#)**
- **[Watch TensorFlow Implementations Walkthrough](#)**

(The links will be updated once the videos are uploaded.)

---

## üìù To-Do
‚úÖ Implement TensorFlow versions using Einsum.
‚úÖ Include functional API and high-level API versions.
‚úÖ Upload Colab links and execution results.
‚úÖ Add additional experiments and insights.

---

## üîó References
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/)
- [TensorFlow Documentation](https://www.tensorflow.org/api_docs)
- [Google Colab](https://colab.research.google.com/)

---

**Author:** Roshini Joga üöÄ

For any questions, feel free to reach out! üéØ

